# Gui√≥n para Video de 90 Segundos

## T√≠tulo: "Arquitectura Data Lakehouse con CDC y Apache Iceberg: Potenciado por Amazon Q Developer"

### [0-10 segundos]
**Presentador:** "Bienvenidos a esta breve presentaci√≥n sobre nuestra arquitectura de Data Lakehouse. Hoy veremos c√≥mo capturamos cambios en bases de datos relacionales y los almacenamos en un formato moderno para an√°lisis."

**Pantalla:** T√≠tulo del video y logo del proyecto Lakehouse.

### [10-20 segundos]
**Presentador:** "¬øSab√≠as que CDC o Change Data Capture puede capturar cambios en bases de datos sin afectar su rendimiento? Esta t√©cnica identifica y captura modificaciones para su procesamiento posterior. Apache Iceberg es un formato de tabla de alto rendimiento que ofrece control de versiones y evoluci√≥n de esquema."

**Pantalla:** Infograf√≠a simple explicando CDC e Iceberg con sus principales ventajas.

### [20-30 segundos]
**Presentador:** "Para desarrollar esta arquitectura, utilizamos Amazon Q Developer, que nos ayud√≥ a generar c√≥digo como este para la integraci√≥n de Spark con Iceberg, ahorrando horas de desarrollo."

**Pantalla:** Captura de pantalla mostrando una consulta a Amazon Q Developer: "Escribe c√≥digo para configurar Spark con Apache Iceberg y S3" y la respuesta generada.

### [30-45 segundos]
**Presentador:** "Nuestra arquitectura se compone de tres capas: fuentes de datos con PostgreSQL, procesamiento de eventos con Debezium y Kafka, y almacenamiento en formato Iceberg en S3 con Spark streaming."

**Pantalla:** Diagrama de arquitectura completo, resaltando cada capa mientras se menciona.

### [45-55 segundos]
**Presentador:** "¬øSab√≠as que Apache Iceberg permite viajes en el tiempo a trav√©s de tus datos? Debezium captura los cambios en PostgreSQL mediante CDC y los env√≠a a Kafka. Spark Streaming consume estos eventos y los escribe en tablas Iceberg en S3, preservando todo el historial de cambios."

**Pantalla:** Zoom al flujo PostgreSQL ‚Üí Debezium ‚Üí Kafka ‚Üí Spark ‚Üí S3/Iceberg con una visualizaci√≥n de "time travel" en datos.

### [55-70 segundos]
**Presentador:** "Veamos una demostraci√≥n r√°pida. Al insertar datos en PostgreSQL, Amazon Q Developer nos ayud√≥ a generar el c√≥digo para capturar estos eventos y procesarlos autom√°ticamente hasta llegar a nuestra tabla Iceberg."

**Pantalla:** Terminal dividida mostrando:
1. Inserci√≥n en PostgreSQL: `INSERT INTO users (name) VALUES ('Juan Garc√≠a');`
2. C√≥digo generado por Amazon Q Developer para procesar eventos
3. Consulta a tabla Iceberg mostrando los datos capturados

### [70-80 segundos]
**Presentador:** "¬øSab√≠as que Amazon Q Developer puede reducir hasta un 70% el tiempo de desarrollo? Fue crucial para resolver desaf√≠os como la configuraci√≥n de Spark con S3 y la escritura en Iceberg, tareas que normalmente tomar√≠an d√≠as."

**Pantalla:** Fragmento de c√≥digo generado por Amazon Q Developer para la escritura en Iceberg:
```python
def write_to_iceberg(batch_df, batch_id, spark):
    batch_df.write.format("iceberg").mode("append").save("lakehouse_db.raw_events_iceberg")
```

### [80-90 segundos]
**Presentador:** "Esta arquitectura nos permite construir un data lakehouse moderno, combinando lo mejor de los data lakes y data warehouses: escalabilidad, flexibilidad y rendimiento. Gracias a Amazon Q Developer, pudimos implementarla en una fracci√≥n del tiempo habitual."

**Pantalla:** Diagrama completo con un banner: "Desarrollo acelerado con Amazon Q Developer" y los beneficios clave del sistema.

# Diagrama de Arquitectura del Sistema Lakehouse

```mermaid
---
config:
  theme: neutral
  look: classic
  layout: elk
---
flowchart TD
    subgraph subGraph0["Fuentes de Datos"]
        PG["üêò PostgreSQL"]
        DG["‚öôÔ∏è Data Generator"]
    end
    subgraph subGraph1["Procesamiento de Eventos"]
        DEB["üìä Debezium"]
        KAFKA["üöÄ Kafka"]
        SPARK["‚ú® Spark"]
    end
    subgraph subGraph2["Data Lakehouse"]
        S3["‚òÅÔ∏è S3 / LocalStack"]
        ICEBERG["‚ùÑÔ∏è Iceberg"]
    end
    PG -- Cambios en datos --> DEB
    DEB -- CDC Events --> KAFKA
    KAFKA -- Streaming --> SPARK
    SPARK -- Escritura --> S3
    SPARK -- Consulta --> ICEBERG
    S3 -- Almacenamiento --> ICEBERG
    DG -- Inserta/Actualiza/Elimina --> PG
    PG:::database
    DEB:::processing
    KAFKA:::processing
    SPARK:::processing
    S3:::storage
    ICEBERG:::database
    classDef database fill:#f9f,stroke:#333,stroke-width:2px
    classDef processing fill:#bbf,stroke:#333,stroke-width:2px
    classDef storage fill:#bfb,stroke:#333,stroke-width:2px
```

## Descripci√≥n de los Componentes

### Fuentes de Datos
- **PostgreSQL**: Base de datos relacional que almacena los datos transaccionales.
- **Data Generator**: Servicio que genera datos de prueba insertando, actualizando y eliminando registros en PostgreSQL.

### Procesamiento de Eventos
- **Debezium Connector**: Captura los cambios en la base de datos PostgreSQL (CDC - Change Data Capture).
- **Kafka**: Sistema de mensajer√≠a que recibe los eventos CDC y los distribuye.
- **Spark Streaming**: Procesa los eventos en tiempo real desde Kafka.

### Data Lakehouse
- **S3 (LocalStack)**: Almacenamiento de objetos compatible con S3 para guardar los datos en formato Iceberg.
- **Tabla Iceberg**: Formato de tabla de c√≥digo abierto que proporciona control de versiones, evoluci√≥n de esquema y consultas r√°pidas.

## Flujo de Datos

1. El **Data Generator** inserta, actualiza o elimina registros en la tabla `users` de **PostgreSQL**.
2. **Debezium** (a trav√©s de Kafka Connect) captura estos cambios y los publica como eventos CDC en un topic de **Kafka**.
3. **Spark Streaming** consume estos eventos desde Kafka.
4. Spark procesa los eventos y los escribe en formato **Iceberg** en **S3** (LocalStack).
5. Los datos almacenados en formato Iceberg pueden ser consultados posteriormente mediante Spark.

## Interacci√≥n entre Componentes

- **PostgreSQL ‚Üí Debezium**: PostgreSQL est√° configurado con `wal_level=logical` para permitir la replicaci√≥n l√≥gica que Debezium utiliza para capturar cambios.
- **Debezium ‚Üí Kafka**: Los eventos CDC se publican en el topic `postgres.public.users`.
- **Kafka ‚Üí Spark**: Spark Streaming consume los eventos del topic de Kafka.
- **Spark ‚Üí S3**: Spark escribe los eventos en formato Iceberg en S3.
- **S3 ‚Üí Iceberg**: Los metadatos y datos de las tablas Iceberg se almacenan en S3.