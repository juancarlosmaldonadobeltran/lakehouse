# Gui√≥n para Video de 90 Segundos

## T√≠tulo: "Arquitectura Data Lakehouse con CDC y Apache Iceberg: Potenciado por Amazon Q Developer"

### [0-10 segundos]
**Presentador:** "Hola a todos. Hoy les voy a contar c√≥mo crear un data lakehouse realmente potente. Lo interesante es que lo hicimos en tiempo r√©cord gracias a Amazon Q Developer. Vamos a ver c√≥mo capturamos cambios en una base de datos relacional y los transformamos en algo realmente √∫til."

**Pantalla:** T√≠tulo del video y logo del proyecto Lakehouse con estilo moderno.

### [10-20 segundos]
**Presentador:** "¬øSab√≠as que CDC o Change Data Capture funciona como una c√°mara de seguridad para tu base de datos? Captura todos los cambios en los datos sin afectar el rendimiento de la base de datos. Por otra parte, Apache Iceberg es como un sistema de control de versiones para tus datos. Imagina poder volver a cualquier punto en el tiempo de tu informaci√≥n."

**Pantalla:** Infograf√≠a clara explicando CDC e Iceberg con sus principales ventajas.

### [20-30 segundos]
**Presentador:** "Le ped√≠ a Amazon Q Developer que me ayudara con el c√≥digo para conectar Spark con Iceberg y en segundos me gener√≥ la soluci√≥n. Me ahorr√≥ d√≠as de trabajo."

**Pantalla:** Captura de pantalla mostrando una conversaci√≥n real con Amazon Q Developer con la respuesta generada.

### [30-45 segundos]
**Presentador:** "La arquitectura propuesta es muy eficiente: tenemos Postgres guardando los datos, Debezium y Kafka moviendo los cambios en tiempo real, y todo termina en tablas de Iceberg en S3 generando una l√≠nea de producci√≥n de datos bien definida."

**Pantalla:** Diagrama de arquitectura con los iconos mostrando el flujo de datos.

### [45-55 segundos]
**Presentador:** "¬øSab√≠as que con Iceberg puedes pr√°cticamente viajar en el tiempo a trav√©s de tus datos? Es como tener una m√°quina del tiempo. Debezium captura cada cambio en Postgres, Kafka lo distribuye, y Spark lo guarda todo en Iceberg. As√≠ conservas la historia completa de tus datos."

**Pantalla:** Visualizaci√≥n mostrando c√≥mo los datos evolucionan y se pueden consultar en diferentes momentos.

### [55-70 segundos]
**Presentador:** "Veamos esto en acci√≥n. Cuando insertamos datos en Postgres, observa lo que sucede. Todo el camino hasta Iceberg ocurre autom√°ticamente. Y todo este c√≥digo lo gener√≥ Amazon Q Developer usando algunos prompts."

**Pantalla:** Terminal con comandos ejecut√°ndose, mostrando el flujo completo de datos con visualizaciones claras.

### [70-80 segundos]
**Presentador:** "¬øSab√≠as que Amazon Q Developer me ahorra un gran porcentaje del tiempo? Cuando me enfrent√© a la  configuraci√≥n de Spark con Iceberg y S3, simplemente le pregunt√© a Q y obtuve la soluci√≥n. Resuelto en minutos lo que normalmente me llevar√≠a d√≠as."

**Pantalla:** Comparaci√≥n visual de "Antes vs Despu√©s de Amazon Q Developer" mostrando el c√≥digo generado con un contador de tiempo ahorrado.

### [80-90 segundos]
**Presentador:** "Y con esto concluimos. Un data-lakehouse moderno construido en tiempo r√©cord. Escalable, flexible y de alto rendimiento. Amazon Q Developer fue como tener un experto en cada tecnolog√≠a disponible. El resultado: semanas de trabajo completadas en d√≠as."

**Pantalla:** Diagrama final con un banner: "Desarrollo acelerado con Amazon Q Developer" y los beneficios clave resaltados con iconos profesionales.

# Diagrama de Arquitectura del Sistema Lakehouse

```mermaid
---
config:
  theme: neutral
  look: classic
  layout: elk
---
flowchart TD
    subgraph subGraph0["Fuentes de Datos"]
        PG["üêò PostgreSQL"]
        DG["‚öôÔ∏è Data Generator"]
    end
    subgraph subGraph1["Procesamiento de Eventos"]
        DEB["üìä Debezium"]
        KAFKA["üöÄ Kafka"]
        SPARK["‚ú® Spark"]
    end
    subgraph subGraph2["Data Lakehouse"]
        S3["‚òÅÔ∏è S3 / LocalStack"]
        ICEBERG["‚ùÑÔ∏è Iceberg"]
    end
    PG -- Cambios en datos --> DEB
    DEB -- CDC Events --> KAFKA
    KAFKA -- Streaming --> SPARK
    SPARK -- Escritura --> S3
    SPARK -- Consulta --> ICEBERG
    S3 -- Almacenamiento --> ICEBERG
    DG -- Inserta/Actualiza/Elimina --> PG
    PG:::database
    DEB:::processing
    KAFKA:::processing
    SPARK:::processing
    S3:::storage
    ICEBERG:::database
    classDef database fill:#f9f,stroke:#333,stroke-width:2px
    classDef processing fill:#bbf,stroke:#333,stroke-width:2px
    classDef storage fill:#bfb,stroke:#333,stroke-width:2px
```

## Descripci√≥n de los Componentes

### Fuentes de Datos
- **PostgreSQL**: Base de datos relacional que almacena los datos transaccionales.
- **Data Generator**: Servicio que genera datos de prueba insertando, actualizando y eliminando registros en PostgreSQL.

### Procesamiento de Eventos
- **Debezium Connector**: Captura los cambios en la base de datos PostgreSQL (CDC - Change Data Capture).
- **Kafka**: Sistema de mensajer√≠a que recibe los eventos CDC y los distribuye.
- **Spark Streaming**: Procesa los eventos en tiempo real desde Kafka.

### Data Lakehouse
- **S3 (LocalStack)**: Almacenamiento de objetos compatible con S3 para guardar los datos en formato Iceberg.
- **Tabla Iceberg**: Formato de tabla de c√≥digo abierto que proporciona control de versiones, evoluci√≥n de esquema y consultas r√°pidas.

## Flujo de Datos

1. El **Data Generator** inserta, actualiza o elimina registros en la tabla `users` de **PostgreSQL**.
2. **Debezium** (a trav√©s de Kafka Connect) captura estos cambios y los publica como eventos CDC en un topic de **Kafka**.
3. **Spark Streaming** consume estos eventos desde Kafka.
4. Spark procesa los eventos y los escribe en formato **Iceberg** en **S3** (LocalStack).
5. Los datos almacenados en formato Iceberg pueden ser consultados posteriormente mediante Spark.

## Interacci√≥n entre Componentes

- **PostgreSQL ‚Üí Debezium**: PostgreSQL est√° configurado con `wal_level=logical` para permitir la replicaci√≥n l√≥gica que Debezium utiliza para capturar cambios.
- **Debezium ‚Üí Kafka**: Los eventos CDC se publican en el topic `postgres.public.users`.
- **Kafka ‚Üí Spark**: Spark Streaming consume los eventos del topic de Kafka.
- **Spark ‚Üí S3**: Spark escribe los eventos en formato Iceberg en S3.
- **S3 ‚Üí Iceberg**: Los metadatos y datos de las tablas Iceberg se almacenan en S3.