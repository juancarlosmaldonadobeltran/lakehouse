services:
  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3
      - AWS_DEFAULT_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test123
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./localstack/localstack_setup.sh:/etc/localstack/init/ready.d/localstack_setup.sh"
      
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
      
  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  postgres:
    image: postgres:13
    container_name: postgres-transactions
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=transactions
    command: [
      "postgres", 
      "-c", "wal_level=logical",
      "-c", "max_wal_senders=8",
      "-c", "max_replication_slots=4"
    ]
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/initdb.sql:/docker-entrypoint-initdb.d/initdb.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d transactions"]
      interval: 5s
      timeout: 5s
      retries: 5
      
  # Kafka Connect con plugins de Debezium para CDC
  kafka-connect:
    build:
      context: ./kafka-connect
      dockerfile: Dockerfile
    container_name: kafka-connect
    depends_on:
      - postgres
      - kafka
      - zookeeper
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    volumes:
      - ./kafka-connect:/tmp/kafka-connect
      
  data-generator:
    image: python:3.9-slim
    container_name: data-generator
    depends_on:
      - postgres
      - kafka-connect
    volumes:
      - ./data-generator:/app
    working_dir: /app
    command: ["/bin/bash", "-c", "pip install -r requirements.txt && python generate_data.py"]
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=transactions
      
  # PySpark streaming job para consumir eventos CDC
  spark-streaming:
    image: bitnami/spark:3.3.0
    container_name: spark-streaming
    depends_on:
      - kafka
      - kafka-connect
    volumes:
      - ./spark-job:/opt/bitnami/spark/app
    command: >
      /bin/bash -c "
        echo 'Waiting for Kafka to be ready...' &&
        sleep 30 &&
        echo 'Installing required packages...' &&
        pip install kafka-python==2.0.2 &&
        echo 'Starting PySpark streaming job...' &&
        /opt/bitnami/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 /opt/bitnami/spark/app/cdc_consumer.py
      "

volumes:
  postgres_data: